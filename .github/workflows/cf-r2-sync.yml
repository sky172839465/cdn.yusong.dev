name: Sync Files to Cloudflare R2

on:
  push:
    branches: ["main"]

env:
  R2_ENDPOINT: https://73f26f76b356e07a46be5fa1485600b7.r2.cloudflarestorage.com
  R2_BUCKET: cdn
  SYNC_DIR: cdn

concurrency:
  group: "sync-r2"
  cancel-in-progress: true

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # needed for diff detection

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: latest

      # - name: Set up AWS CLI for R2
      #   uses: aws-actions/configure-aws-credentials@v5.1.0
      #   with:
      #     aws-access-key-id: ${{ secrets.R2_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      #     aws-region: us-east-1

      # configure-aws-credentials not support cf keys yet
      - name: Configure AWS credentials for Cloudflare R2
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          mkdir -p ~/.aws
          cat <<EOF > ~/.aws/credentials
          [default]
          aws_access_key_id=${AWS_ACCESS_KEY_ID}
          aws_secret_access_key=${AWS_SECRET_ACCESS_KEY}
          EOF
          echo "[default]" > ~/.aws/config
          echo "region = us-east-1" >> ~/.aws/config
          echo "Configured AWS credentials for R2"

      - name: Detect changes in cdn folder
        id: diff
        run: |
          git fetch origin main
          git diff --name-status HEAD^ HEAD -- "${SYNC_DIR}/" > changes.txt
          echo "Changed files under ${SYNC_DIR}:"
          cat changes.txt || true

      - name: Process file changes (add/update/delete)
        run: |
          while read status file; do
            case "$status" in
              A|M)
                action="A" && [[ "$status" == "M" ]] && action="‚ôªÔ∏è"
                echo "${action} Uploading $file"
                aws s3 cp "$file" "s3://${R2_BUCKET}/${file#${SYNC_DIR}/}" \
                  --endpoint-url $R2_ENDPOINT \
                  --acl public-read \
                  --cache-control "public, max-age=31536000, immutable"
                ;;
              D)
                echo "üóëÔ∏è Removing $file"
                aws s3 rm "s3://${R2_BUCKET}/${file#${SYNC_DIR}/}" --endpoint-url $R2_ENDPOINT
                ;;
              R*)
                old_file=$(echo "$file" | awk '{print $2}')
                new_file=$(echo "$file" | awk '{print $3}')
                echo "üîÅ Rename: remove $old_file, add $new_file"
                aws s3 rm "s3://$R2_BUCKET/${old_file#${SYNC_DIR}/}" --endpoint-url $R2_ENDPOINT
                aws s3 cp "$new_file" "s3://$R2_BUCKET/${new_file#${SYNC_DIR}/}" \
                  --endpoint-url $R2_ENDPOINT \
                  --acl public-read \
                  --cache-control "public, max-age=31536000, immutable"
                ;;
            esac
          done < changes.txt

      - name: Generate ls.json for each folder in SYNC_DIR & upload to R2
        run: |
          if [ ! -s changes.txt ]; then
            echo "No changes detected in ${SYNC_DIR}. Exiting."
            exit 0
          fi

          # function to get image dimensions
          get_image_info() {
            local file="$1"
            if identify "$file" &>/dev/null; then
              read width height <<< $(identify -format "%w %h" "$file")
              echo "{\"name\":\"$(basename "$file")\",\"width\":$width,\"height\":$height}"
            else
              echo "{\"name\":\"$(basename "$file")\"}"
            fi
          }

          # Collect unique directories from changed files
          dirs=$(awk '{print $2}' changes.txt | while read file; do
            # Get directory of file
            dir="$(dirname "$file")"
            echo "$dir"
          done | sort -u)
          
          # Regenerate ls.json only once per directory
          modified_files=()
          for dir in $dirs; do
            # Skip if directory doesn't exist (deleted files)
            [ ! -d "$dir" ] && continue
          
            files_json=""
            for file in "$dir"/*; do
              [ ! -f "$file" ] && continue
              # only process if regular file
              files_json+=$(node getImageSize.js "$file")
              files_json+=", "
            done
          
            files_json=$(echo "$files_json" | sed 's/, $//')
            [ -z "$files_json" ] && continue
          
            json="{\"path\": \"${dir#$SYNC_DIR/}/\", \"files\": [${files_json}]}"
            echo "$json" | jq . > "$dir/ls.json"
            echo "Generated $dir/ls.json"
            modified_files+=("$dir/ls.json")
          done

          # upload files
          for file in "${modified_files[@]}"; do
            echo "üìÇ Uploading $file"
            aws s3 cp "$file" "s3://${R2_BUCKET}/${file#${SYNC_DIR}/}" \
              --endpoint-url $R2_ENDPOINT \
              --acl public-read \
              --cache-control "public, max-age=60"
          done
