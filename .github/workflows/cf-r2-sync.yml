name: Sync Files to Cloudflare R2

on:
  push:
    branches: ["main"]

env:
  R2_ENDPOINT: https://73f26f76b356e07a46be5fa1485600b7.r2.cloudflarestorage.com
  R2_BUCKET: cdn
  SYNC_DIR: cdn

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # needed for diff detection

      # - name: Set up AWS CLI for R2
      #   uses: aws-actions/configure-aws-credentials@v5.1.0
      #   with:
      #     aws-access-key-id: ${{ secrets.R2_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      #     aws-region: us-east-1

      # configure-aws-credentials not support cf keys yet
      - name: Configure AWS credentials for Cloudflare R2
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          mkdir -p ~/.aws
          cat <<EOF > ~/.aws/credentials
          [default]
          aws_access_key_id=${AWS_ACCESS_KEY_ID}
          aws_secret_access_key=${AWS_SECRET_ACCESS_KEY}
          EOF
          echo "[default]" > ~/.aws/config
          echo "region = us-east-1" >> ~/.aws/config
          echo "Configured AWS credentials for R2"

      - name: Detect changes in cdn folder
        id: diff
        run: |
          git fetch origin main
          git diff --name-status HEAD^ HEAD -- "${SYNC_DIR}/" > changes.txt
          echo "Changed files under ${SYNC_DIR}:"
          cat changes.txt || true

      - name: Process file changes (add/update/delete)
        run: |
          while read status file; do
            case "$status" in
              A|M)
                action="A" && [[ "$status" == "M" ]] && action="‚ôªÔ∏è"
                echo "${action} Uploading $file"
                aws s3 cp "$file" "s3://${R2_BUCKET}/${file#${SYNC_DIR}/}" \
                  --endpoint-url $R2_ENDPOINT \
                  --acl public-read \
                  --cache-control "public, max-age=31536000, immutable"
                ;;
              D)
                echo "‚ùå Removing $file"
                aws s3 rm "s3://${R2_BUCKET}/${file#${SYNC_DIR}/}" --endpoint-url $R2_ENDPOINT
                ;;
              R*)
                old_file=$(echo "$file" | awk '{print $2}')
                new_file=$(echo "$file" | awk '{print $3}')
                echo "üîÅ Rename: remove $old_file, add $new_file"
                aws s3 rm "s3://$R2_BUCKET/${old_file#${SYNC_DIR}/}" --endpoint-url $R2_ENDPOINT
                aws s3 cp "$new_file" "s3://$R2_BUCKET/${new_file#${SYNC_DIR}/}" \
                  --endpoint-url $R2_ENDPOINT \
                  --acl public-read \
                  --cache-control "public, max-age=31536000, immutable"
                ;;
            esac
          done < changes.txt

      - name: Generate ls.json for each folder in SYNC_DIR & upload to R2
        run: |
          if [ ! -d "${SYNC_DIR}" ]; then
            exit 0
          fi
          # generate files
          find "${SYNC_DIR}" -type d | while read dir; do
            files=$(find "$dir" -maxdepth 1 -type f -not -name "ls.json" -printf '"%f", ' | sed 's/, $//')
            [ -z "$files" ] && continue
            json="{\"path\": \"${dir#$SYNC_DIR/}/\", \"files\": [${files}]}"
            echo "$json" | jq . > "$dir/ls.json"
            echo "Generated $dir/ls.json"
          done
          # upload files
          find "${SYNC_DIR}" -name "ls.json" -type f | while read file; do
            echo "üìÇ Uploading $file"
            aws s3 cp "$file" "s3://${R2_BUCKET}/${file#${SYNC_DIR}/}" \
              --endpoint-url $R2_ENDPOINT \
              --acl public-read \
              --cache-control "public, max-age=60"
          done
